#Text mining with R tutorial examples

#================#SECTION 1#================#
#=============#Using tidy data#=============#
#===========================================#

library(dplyr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(stringr)
library(janeaustenr)
library(gutenbergr)

#text to change
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

#change text from two lines into 4, (change into data frame)
text_df <- data_frame(line = 1:4, text = text)

#tokenizatoin, use 'to_lower = FALSE' to remove auto lower case
text_df %>%
  unnest_tokens(word,text)


##EXAMPLE## 

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()

#tokenize each line into words in seperate rows
tidy_books <- original_books %>%
  unnest_tokens(word,text)

#call stop words dictionary
data(stop_words)

#remove stop words from dataframe
tidy_books <- tidy_books %>%
  anti_join(stop_words)

#most common words after removal of stop words
tidy_books %>%
  count(word,sort = TRUE)

#easy to now pipe into ggplot for visualisation:
#data for displaying most common words above 600 uses
tidy_books %>%
  count(word,sort = TRUE) %>%
  filter(n>600) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(word,n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

##EXAMPLE 2
#download texts from project gutenberg, numbers are novel IDs
hgwells <- gutenberg_download(c(35,36,5230,159))

#tidy data
tidy_hgwells <- hgwells %>%
  #sepearte into tokens
  unnest_tokens(word, text) %>%
  #remove stop words
  anti_join(stop_words)

#check most used words across novels downlaoded above...
tidy_hgwells %>%
  count(word,sort = TRUE)


#download third data set of novels
bronte <- gutenberg_download(c(1260,768,969,9182,767))
#again tidy up
tidy_bronte <- bronte %>%
  unnest_tokens(word,text) %>%
  anti_join(stop_words)

tidy_bronte %>%
  count(word,sort = TRUE)


#now we have three sets of data (novels from dif authers) we can compare against each
frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  #stre_extract is used to make "_any_" same as "any" for analysis purposes
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)

#correlation test against authors
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)
#0.7609938
cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         ~ proportion + `Jane Austen`)
#0.4241601
#=> more correlated between Austen and Bronte than Austen and Wells



#================#SECTION 2#================#
#============#Sentiment analysis#===========#
#===========================================#

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)


nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)


jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
#plot results
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")


#word cloud
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))



#================#SECTION 3#================#
#============#Frequency: tf-idf#============#
#===========================================#

book_words <- austen_books() %>%
  unnest_tokens(word,text) %>%
  count(book,word,sort = TRUE) %>%
  ungroup()

total_words <- book_words %>%
  group_by(book) %>%
  summarise(total = sum(n))

book_words <- left_join(book_words,total_words)

#n is number of times it appears in the book, total is number of words in the book
#ranking of frequency of words per book
book_words

ggplot(book_words,aes(n/total,fill=book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA,0.0009) +
  facet_wrap(~book,ncol = 2,scales = "free_y")

#Zipf's law
freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)
#plot results after table
freq_by_rank
#each of the novels follows an inversely proportional relationhip with negative 
#constant slope
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()


#using the bind_tf_idf function to weigh words
book_words <- book_words %>%
  bind_tf_idf(word, book, n)
book_words

#look at high tf-idf terms
book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
#visualise results
book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()
#all nouns, =>similar language usage throughout the novels, only difference is characters
#and places.

#EXAMPLE 2-PHYSICS TEXTS
#download texts
physics <- gutenberg_download(c(37729, 14725, 13476, 5001), 
                              meta_fields = "author")
#unnest into tokens
physics_words <- physics %>%
  unnest_tokens(word, text) %>%
  count(author, word, sort = TRUE) %>%
  ungroup()
#can see that we need to get rid of common words again
physics_words
#calculate tf-idf
plot_physics <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan", 
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

plot_physics %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip()
#some errors in the text so can look up and decide if needs deletion
physics %>% 
  filter(str_detect(text, "eq\\.")) %>% 
  select(text)
physics %>% 
  filter(str_detect(text, "K1")) %>% 
  select(text)
#Look up each case individually and decide if it is important, also co-ord
#add new words to exclude
mystopwords <- data_frame(word = c("eq", "co", "rc", "ac", "ak", "bn", 
                                   "fig", "file", "cg", "cb", "cm"))
physics_words <- anti_join(physics_words, mystopwords, by = "word")
plot_physics <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(author) %>% 
  top_n(15, tf_idf) %>%
  ungroup %>%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan",
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

ggplot(plot_physics, aes(word, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip()



#================#SECTION 4#================#
#=================#n-grams#=================#
#===========================================#
#setting n=2 we are looking at pairs of consecutive words (bigrams)
austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

#shows them in order
austen_bigrams

#look up most common
austen_bigrams %>%
  count(bigram, sort = TRUE)

#we can use seperate() to split words to remove 'stop-words'
bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

#updated set of bigrams without stop words
bigram_counts

#in some analyses it may be important to recombine for further analysis using unite()
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united

#May sometimes be interested in using trigrams (n=3)
#same concept as before just using n=3
austen_books() %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

#using the seperated format we can look up things:such as the most common streets
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)

#like before, can also incoperate tf-idf into it to compare against books
bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf

#Sentiment analysis using bigrams
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

not_words
